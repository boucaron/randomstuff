Superintelligence: Paths, Dangers, Strategies by Nick Bostrom

I had a first initial quick read and I was not in the mood.
I mean that I was not concentrated enough to really dive into and start to appreciate it.
So far, I consummed a few chapters, I am still skeptical at the current time (2023), but not strongly skeptical.
First, there is a short introduction to AI and a quick history.
Then, it talks about various kind of improvements in various fields biological, chips, simulation, algorithms ...etc...
The current limits of the state of the art back to 2015 are well explained.
I agree there is really something about various "compounding" effects which makes sense when you know the exponential improvements in the chips.
Weak AI is already a reality in more and more fields: it can be stupid (or even more stupid) than a human and also very dangerous.
For Strong AI I do not know, I think the main issue is 'algorithmic': it is a human problem.
There is also an hardware issue which is less important overtime, will we have stagnation on this side, possibly.
Of course it is just my feeling, there are complex interaction and feedback loops that will improve and it will provide speed-up increments, but is it an exponential speed-up outside of the hardware part, I don't know.
Strong AI is a problem with combinatorial explosion in several dimensions, even thousands or higher order of magnitude on the hardware are still just few atoms in a problem bigger than the size of the universe. Let me rephrase: it will not change 'too much' with a better hardware something you cannot solve in several lifetimes: so the 'algorithmic' issue is the true human problem.
Of course existing Weak AI can help to accelerate humans to tackle the Strong AI problem and most of the time will improve or create better Weaker AIs. However, I have to ask the question, does such Weak AI provides to the human an improved intelligence, up to a point yes, but is it enough to the Strong AI problem, I am not sure. 

We are already different from persons from 50 years ago.
I mean we already have a few Weak AI in someways, we have a fast and easy access to a huge amount of information that was not even thinkable for our grand-parents. We can know more than all of our ancestors together but are we really more intelligent, I don't think the answer to this question is so obvious. Also as the size of our specy grows very much, we can also say that we have much more geniuses than during the medieval ages, so yes it is a potential acceleration factor. Let say push further in this way, and let say that we have external Weak AI accelerators to further improve our slow biological brain, even so will we be able to reach the intelligence plateau required to solve the 'algorithm' issue to the Strong AI, for sure it will be faster but will it be enough ?
Or will we have already burnt all our ressources and made our specy extinct ?

I think something is missing in this book. Basically the Strong AI will be built using several Weak AIs and humans knowledge. So humans will be powered too by the same Weak AIs, with the same technology.
Let say, the 'pivot'/'plateau' is reached by a Strong AI, similarly we will be powered too by Weak AIs composing it, so yes, the Strong AI being digital will be much much faster than ourselves and it will be able to evolve at a much higher speed provided it has an access to resources: energy, communication and physical means to improve itself and/or to harm ourselves. So I think it not so obvious that we will have a fast transition. The information being more and more distribued, I do not believe in the theory of the 'singleton', if you check history, you will see that many discovery/invention was made in the world nearly during the same time, of course there is the acceleration factor due to money and the talent of persons, but information is too widespread to have only one person being able to reach it.

The book explores many interesting questions 










