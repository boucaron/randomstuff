Superintelligence: Paths, Dangers, Strategies by Nick Bostrom

NB: I still have some strong bias that Superintelligence is feasible: it does not mean that I disagree on the reasonning of the author assuming Superintelligence is there and also the implication and various topics discussed. I think this is probably one of the more interesting part of this book.

First Iteration, I am skeptical through the exponential complexity and also we are reaching limits on the hardware side.

I had a first initial quick read and I was not in the mood.
I mean that I was not concentrated enough to really dive into and start to appreciate it.
So far, I consummed a few chapters, I am still skeptical at the current time (2023), but not strongly skeptical.
First, there is a short introduction to AI and a quick history.
Then, it talks about various kind of improvements in various fields biological, chips, simulation, algorithms ...etc...
The current limits of the state of the art back to 2015 are well explained.
I agree there is really something about various "compounding" effects which makes sense when you know the exponential improvements in the chips.
Weak AI is already a reality in more and more fields: it can be stupid (or even more stupid) than a human and also very dangerous.
For Strong AI I do not know, I think the main issue is 'algorithmic': it is a human problem.
There is also an hardware issue which is less important overtime, will we have stagnation on this side, possibly.
Of course it is just my feeling, there are complex interaction and feedback loops that will improve and it will provide speed-up increments, but is it an exponential speed-up outside of the hardware part, I don't know.
Strong AI is a problem with combinatorial explosion in several dimensions, even thousands or higher order of magnitude on the hardware are still just few atoms in a problem bigger than the size of the universe. Let me rephrase: it will not change 'too much' with a better hardware something you cannot solve in several lifetimes: so the 'algorithmic' issue is the true human problem.
Of course existing Weak AI can help to accelerate humans to tackle the Strong AI problem and most of the time will improve or create better Weaker AIs. However, I have to ask the question, does such Weak AI provides to the human an improved intelligence, up to a point yes, but is it enough to the Strong AI problem, I am not sure.

Homo techno not anymore Homo Sapiens Sapiens

We are already different from persons from 50 years ago.
I mean we already have a few Weak AI in someways, we have a fast and easy access to a huge amount of information that was not even thinkable for our grand-parents. We can know more than all of our ancestors together but are we really more intelligent, I don't think the answer to this question is so obvious. Also as the size of our specy grows very much, we can also say that we have much more geniuses than during the medieval ages, so yes it is a potential acceleration factor. Let say push further in this way, and let say that we have external Weak AI accelerators to further improve our slow biological brain, even so will we be able to reach the intelligence plateau required to solve the 'algorithm' issue to the Strong AI, for sure it will be faster but will it be enough ?
Or will we have already burnt all our ressources and made our specy extinct ?

I think something is missing in this book. Basically the Strong AI will be built using several Weak AIs and humans knowledge. So humans will be powered too by the same Weak AIs, with the same technology.
Let say, the 'pivot'/'plateau' is reached by a Strong AI, similarly we will be powered too by Weak AIs composing it, so yes, the Strong AI being digital will be much much faster than ourselves and it will be able to evolve at a much higher speed provided it has an access to resources: energy, communication and physical means to improve itself and/or to harm ourselves. So I think it not so obvious that we will have a fast transition. The information being more and more distribued, I do not believe in the theory of the 'singleton', if you check history, you will see that many discovery/invention was made in the world nearly during the same time, of course there is the acceleration factor due to money and the talent of persons, but information is too widespread to have only one person being able to reach it. 

The book explores many interesting area on how can we keep the control of something more complex than the whole humanity. This is probably the most interesting part of the book. It explores various cases and scenarios on how such Strong AI can be controlled, how it will affect our societies: poor, rich, capital, no capital, common.

I did not like too much being compared to an horse transitioning from/to the automotive revolution in this new age: meaning I will be useless to the economy as a whole. The first difference as a specy, we are not passive with respect to what may happen to us.
The 'classic' scenario is the small subset of rich people will replace human force, automation as usual: from the mechanical looms to the robots used today. Yes, it is happening. But actually, this is kind of funny, because if the StrongAI starts to work for itself from an economy perspective it will targets for sure where there is most of the money to be made quickly, or to disrupt the human society as a whole. It is simpler for a Stronger AI to automatize an already mostly digital process where data are already spread-out on the Internet, just think for instance of any kind of data analytics jobs, it is the first easy target, because all last decades improvements in AI are exactly on this subject in one way or the other.

Let us go further, there is already a problem with metal resources, may be a big growth of robots will be feasible for few decades but not so much. Of course technology will evolves, there will be less need for metal replaced by something else mineral/organic/whatever, but we remain on finite resources so it is the same ending: growth, stagnation, slow-down.
Something related to this is already there at the human level: the limitation of the head counts is  hapening is several western countries (Japan, Germany ...etc...) due to various reasons: fertility issues, not wanting children, cost of life due to the economic situation, having only one child to provide her/him a larger capita to start in life.

Through history, we know 'evolution/progress' was not something good for the individual or the human specy in general. But, I think nearly anyone making a prediction about what will happen above 2 to 3 years will be very wrong most of the time: for instance, let think about flighting-cars predicted in the 50's or today, technical feasability yes may be, but more seriously given the amount of energy required to make it works, in a scarcity situation on the energy, you can forget about it. It is a total non-sense.
If you have only bots, what economy do you expect to have ? A class of humans expect to be in control having nearly all the resources and the remaining humans will be poors. What do you expect ? Humans being meat, pet, art ?










